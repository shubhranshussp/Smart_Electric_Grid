        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "except ImportError:  # pragma: no cover\n",
        "    files = None\n",
        "\n",
        "DATASET_PATH = Path(\"Dataset.csv\")\n",
        "\n",
        "if DATASET_PATH.exists():\n",
        "    print(f\"Dataset ready at: {DATASET_PATH.resolve()}\")\n",
        "elif files is not None:\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise RuntimeError(\"No file uploaded.\")\n",
        "    first_key = next(iter(uploaded))\n",
        "    DATASET_PATH.write_bytes(uploaded[first_key])\n",
        "    print(f\"Loaded {first_key} -> {DATASET_PATH.name}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Dataset.csv not found and google.colab is unavailable.\")\n"
      "outputs": []
        "from pathlib import Path\n",
        "\n",
        "pd.options.display.float_format = \"{:.2f}\".format\n",
        "\n",
        "DATA_PATH = Path(\"Dataset.csv\")\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\"Dataset.csv is required. Run the upload cell first.\")\n",
        "\n",
        "column_names = [\n",
        "    \"date\",\n",
        "    \"time\",\n",
        "    \"voltage\",\n",
        "    \"current\",\n",
        "    \"power_factor\",\n",
        "    \"load_kw\",\n",
        "    \"weekend_flag\",\n",
        "    \"season\",\n",
        "    \"temperature_f\",\n",
        "    \"humidity\",\n",
        "raw = pd.read_csv(\n",
        "    DATA_PATH,\n",
        "    usecols=range(len(column_names)),\n",
        "    names=column_names,\n",
        "    header=0,\n",
        "    engine=\"python\",\n",
        ")\n",
        "def clean_time(value: str) -> str:\n",
        "    if pd.isna(value):\n",
        "        return np.nan\n",
        "    value = str(value).strip()\n",
        "    if not value or value.lower() == \"nan\":\n",
        "        return np.nan\n",
        "    value = value.replace(\"-\", \":\").replace(\".\", \":\")\n",
        "    parts = value.split(\":\")\n",
        "    if len(parts) == 1:\n",
        "        return f\"{parts[0].zfill(2)}:00\"\n",
        "    return f\"{parts[0].zfill(2)}:{parts[1].zfill(2)}\"\n",
        "\n",
        "raw[\"date\"] = raw[\"date\"].replace(\"\", np.nan).ffill()\n",
        "raw[\"time\"] = raw[\"time\"].apply(clean_time).fillna(\"00:00\")\n",
        "\n",
        "datetime_series = pd.to_datetime(\n",
        "    raw[\"date\"] + \" \" + raw[\"time\"],\n",
        "    dayfirst=True,\n",
        "    errors=\"coerce\",\n",
        ")\n",
        "valid_mask = datetime_series.notna()\n",
        "df = raw.loc[valid_mask].copy()\n",
        "datetime_index = pd.DatetimeIndex(datetime_series[valid_mask])\n",
        "datetime_index = datetime_index.tz_localize(\n",
        "    \"Asia/Kolkata\",\n",
        "    nonexistent=\"shift_forward\",\n",
        "    ambiguous=\"infer\",\n",
        ").tz_convert(\"Asia/Kolkata\").tz_localize(None)\n",
        "\n",
        "df.index = datetime_index\n",
        "df = df[~df.index.duplicated(keep=\"first\")].sort_index()\n",
        "df.drop(columns=[\"date\", \"time\"], inplace=True)\n",
        "df.rename(columns={\"weekend_flag\": \"is_weekend\"}, inplace=True)\n",
        "\n",
        "numeric_cols = [\"voltage\", \"current\", \"power_factor\", \"load_kw\", \"temperature_f\", \"humidity\"]\n",
        "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "df[\"is_weekend\"] = (\n",
        "    pd.to_numeric(df.get(\"is_weekend\"), errors=\"coerce\")\n",
        "    .fillna(0)\n",
        "    .clip(lower=0)\n",
        "    .round()\n",
        "    .astype(int)\n",
        ")\n",
        "df[\"season\"] = pd.to_numeric(df.get(\"season\"), errors=\"coerce\")\n",
        "full_index = pd.date_range(df.index.min(), df.index.max(), freq=\"H\")\n",
        "df = df.reindex(full_index)\n",
        "df[\"is_weekend\"] = df[\"is_weekend\"].fillna(method=\"ffill\").fillna(method=\"bfill\").astype(int)\n",
        "df[\"season\"] = (\n",
        "    df[\"season\"]\n",
        "    .fillna(method=\"ffill\")\n",
        "    .fillna(method=\"bfill\")\n",
        "    .astype(int)\n",
        ")\n",
        "df[numeric_cols] = (\n",
        "    df[numeric_cols]\n",
        "    .interpolate(method=\"time\")\n",
        "    .fillna(method=\"ffill\")\n",
        "    .fillna(method=\"bfill\")\n",
        ")\n",
        "low, high = df[\"load_kw\"].quantile([0.01, 0.99])\n",
        "df[\"load_kw\"] = df[\"load_kw\"].clip(lower=low, upper=high)\n",
        "df[\"hour\"] = df.index.hour\n",
        "df[\"day_of_week\"] = df.index.dayofweek\n",
        "df[\"day_of_year\"] = df.index.dayofyear\n",
        "df[\"month\"] = df.index.month\n",
        "df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
        "df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
        "df[\"dow_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
        "df[\"dow_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
        "summary = {\n",
        "    \"rows\": len(df),\n",
        "    \"date_start\": df.index.min(),\n",
        "    \"date_end\": df.index.max(),\n",
        "    \"missing_after_cleaning\": int(df.isna().sum().sum()),\n",
        "}\n",
        "print(f\"Cleaned rows: {summary['rows']:,}\")\n",
        "print(f\"Date range: {summary['date_start']} \u2192 {summary['date_end']}\")\n",
        "print(f\"Remaining missing values: {summary['missing_after_cleaning']}\")\n",
        "\n",
        "df.to_csv(\"cleaned_electric_load_data.csv\", index_label=\"datetime\")\n",
        "print(\"Saved cleaned dataset \u2192 cleaned_electric_load_data.csv\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(df.index, df[\"load_kw\"], color=\"#1f77b4\", linewidth=0.8)\n",
        "ax.set_title(\"Hourly Load After Cleaning\", fontsize=12)\n",
        "ax.set_ylabel(\"Load (kW)\")\n",
        "ax.set_xlabel(\"Datetime\")\n",
        "ax.grid(alpha=0.3)\n",
        "plt.savefig(\"cleaned_load_overview.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      "outputs": []
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)\n",
        "df = pd.read_csv(\"cleaned_electric_load_data.csv\", index_col=\"datetime\", parse_dates=True)\n",
        "df = df.sort_index().asfreq(\"H\")\n",
        "def add_temporal_flags(frame: pd.DataFrame) -> pd.DataFrame:\n",
        "    frame[\"is_peak_hour\"] = frame[\"hour\"].isin([7, 8, 9, 18, 19, 20, 21]).astype(int)\n",
        "    frame[\"is_night\"] = frame[\"hour\"].isin([0, 1, 2, 3, 4, 5]).astype(int)\n",
        "    frame[\"is_working_hours\"] = frame[\"hour\"].between(9, 17).astype(int)\n",
        "    frame[\"weekend_hour\"] = frame[\"is_weekend\"] * frame[\"hour\"]\n",
        "    return frame\n",
        "df[\"load_lag_1h\"] = df[\"load_kw\"].shift(1)\n",
        "df[\"load_lag_24h\"] = df[\"load_kw\"].shift(24)\n",
        "df[\"load_lag_168h\"] = df[\"load_kw\"].shift(168)\n",
        "df[\"load_rolling_mean_3h\"] = df[\"load_kw\"].rolling(window=3, min_periods=1).mean()\n",
        "df[\"load_rolling_mean_6h\"] = df[\"load_kw\"].rolling(window=6, min_periods=1).mean()\n",
        "df[\"load_rolling_mean_24h\"] = df[\"load_kw\"].rolling(window=24, min_periods=1).mean()\n",
        "df[\"load_rolling_std_24h\"] = df[\"load_kw\"].rolling(window=24, min_periods=1).std().fillna(0)\n",
        "df = add_temporal_flags(df)\n",
        "df[\"temp_humidity_index\"] = df[\"temperature_f\"] * (df[\"humidity\"] / 100)\n",
        "df[\"temp_deviation\"] = df[\"temperature_f\"] - df[\"temperature_f\"].rolling(window=168, min_periods=1).mean()\n",
        "df[\"season_temp\"] = df[\"season\"] * df[\"temperature_f\"]\n",
        "df = df.dropna(subset=[\"load_lag_168h\"]).copy()\n",
        "    \"load_lag_1h\",\n",
        "    \"load_lag_24h\",\n",
        "    \"load_lag_168h\",\n",
        "    \"load_rolling_mean_3h\",\n",
        "    \"load_rolling_mean_6h\",\n",
        "    \"load_rolling_mean_24h\",\n",
        "    \"load_rolling_std_24h\",\n",
        "    \"hour\",\n",
        "    \"day_of_week\",\n",
        "    \"month\",\n",
        "    \"is_weekend\",\n",
        "    \"hour_sin\",\n",
        "    \"hour_cos\",\n",
        "    \"dow_sin\",\n",
        "    \"dow_cos\",\n",
        "    \"is_peak_hour\",\n",
        "    \"is_night\",\n",
        "    \"is_working_hours\",\n",
        "    \"weekend_hour\",\n",
        "    \"temperature_f\",\n",
        "    \"humidity\",\n",
        "    \"temp_humidity_index\",\n",
        "    \"temp_deviation\",\n",
        "    \"season_temp\",\n",
        "    \"voltage\",\n",
        "    \"current\",\n",
        "    \"power_factor\",\n",
        "    \"season\",\n",
        "target_col = \"load_kw\"\n",
        "train_end = int(total_rows * 0.70)\n",
        "val_end = int(total_rows * 0.85)\n",
        "print(\"Chronological split:\")\n",
        "print(f\"  Train: {train_data.index.min()} \u2192 {train_data.index.max()} ({len(train_data)} rows)\")\n",
        "print(f\"  Val:   {val_data.index.min()} \u2192 {val_data.index.max()} ({len(val_data)} rows)\")\n",
        "print(f\"  Test:  {test_data.index.min()} \u2192 {test_data.index.max()} ({len(test_data)} rows)\")\n",
        "y_train = train_data[target_col].values\n",
        "y_val = val_data[target_col].values\n",
        "y_test = test_data[target_col].values\n",
        "minmax_scaler = MinMaxScaler()\n",
        "y_train_minmax = y_scaler_minmax.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "y_val_minmax = y_scaler_minmax.transform(y_val.reshape(-1, 1)).ravel()\n",
        "y_test_minmax = y_scaler_minmax.transform(y_test.reshape(-1, 1)).ravel()\n",
        "standard_scaler = StandardScaler()\n",
        "y_train_standard = y_scaler_standard.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "y_val_standard = y_scaler_standard.transform(y_val.reshape(-1, 1)).ravel()\n",
        "y_test_standard = y_scaler_standard.transform(y_test.reshape(-1, 1)).ravel()\n",
        "\n",
        "train_data.to_csv(\"train_data.csv\")\n",
        "val_data.to_csv(\"val_data.csv\")\n",
        "test_data.to_csv(\"test_data.csv\")\n",
        "\n",
        "np.save(\"X_train_minmax.npy\", X_train_minmax)\n",
        "np.save(\"X_val_minmax.npy\", X_val_minmax)\n",
        "np.save(\"X_test_minmax.npy\", X_test_minmax)\n",
        "np.save(\"y_train_minmax.npy\", y_train_minmax)\n",
        "np.save(\"y_val_minmax.npy\", y_val_minmax)\n",
        "np.save(\"y_test_minmax.npy\", y_test_minmax)\n",
        "\n",
        "np.save(\"X_train_standard.npy\", X_train_standard)\n",
        "np.save(\"X_val_standard.npy\", X_val_standard)\n",
        "np.save(\"X_test_standard.npy\", X_test_standard)\n",
        "np.save(\"y_train_standard.npy\", y_train_standard)\n",
        "np.save(\"y_val_standard.npy\", y_val_standard)\n",
        "np.save(\"y_test_standard.npy\", y_test_standard)\n",
        "\n",
        "with open(\"minmax_scaler.pkl\", \"wb\") as f:\n",
        "with open(\"y_scaler_minmax.pkl\", \"wb\") as f:\n",
        "with open(\"standard_scaler.pkl\", \"wb\") as f:\n",
        "with open(\"y_scaler_standard.pkl\", \"wb\") as f:\n",
        "Path(\"feature_names.txt\").write_text(\"\\n\".join(ml_features))\n",
        "print(\"Saved feature names \u2192 feature_names.txt\")\n",
        "print(f\"Feature matrix shapes (train/val/test): {X_train.shape}, {X_val.shape}, {X_test.shape}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(train_data.index, train_data[target_col], label=\"Train\", linewidth=0.8)\n",
        "ax.plot(val_data.index, val_data[target_col], label=\"Validation\", linewidth=0.8)\n",
        "ax.plot(test_data.index, test_data[target_col], label=\"Test\", linewidth=0.8)\n",
        "ax.set_title(\"Data split overview\", fontsize=12)\n",
        "ax.set_ylabel(\"Load (kW)\")\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "plt.savefig(\"data_splits_overview.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      "outputs": []
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "train_df = pd.read_csv(\"train_data.csv\", index_col=\"datetime\", parse_dates=True)\n",
        "val_df = pd.read_csv(\"val_data.csv\", index_col=\"datetime\", parse_dates=True)\n",
        "test_df = pd.read_csv(\"test_data.csv\", index_col=\"datetime\", parse_dates=True)\n",
        "\n",
        "train_series = train_df[\"load_kw\"]\n",
        "val_series = val_df[\"load_kw\"]\n",
        "test_series = test_df[\"load_kw\"]\n",
        "\n",
        "def adf_pvalue(series: pd.Series) -> float:\n",
        "    result = adfuller(series.dropna(), autolag=\"AIC\")\n",
        "    return result[1]\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
        "    y_true = y_true[mask]\n",
        "    y_pred = y_pred[mask]\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-3, None))) * 100\n",
        "    if len(np.unique(y_true)) > 1:\n",
        "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "        ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n",
        "        r2 = 1 - ss_res / ss_tot\n",
        "        r2 = np.nan\n",
        "    return {\"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape, \"R2\": r2}\n",
        "p_value = adf_pvalue(train_series)\n",
        "d = 0 if p_value <= 0.05 else 1\n",
        "print(f\"ADF p-value: {p_value:.4f} \u2192 Using d = {d}\")\n",
        "candidate_orders = [(1, d, 1), (1, d, 2), (2, d, 1), (2, d, 2), (3, d, 1)]\n",
        "best_aic = np.inf\n",
        "for order in candidate_orders:\n",
        "        model = ARIMA(train_series, order=order)\n",
        "        fitted = model.fit()\n",
        "        if fitted.aic < best_aic:\n",
        "            best_aic = fitted.aic\n",
        "    except Exception as err:\n",
        "        print(f\"Order {order} failed: {err}\")\n",
        "if best_order is None:\n",
        "    raise RuntimeError(\"No ARIMA model could be estimated.\")\n",
        "print(f\"Selected ARIMA order: {best_order} (AIC={best_aic:.2f})\")\n",
        "val_model = ARIMA(train_series, order=best_order).fit()\n",
        "val_forecast = val_model.forecast(steps=len(val_series))\n",
        "val_forecast.index = val_series.index\n",
        "val_metrics = compute_metrics(val_series.values, val_forecast.values)\n",
        "print(f\"Validation hourly MAPE: {val_metrics['MAPE']:.2f}%\")\n",
        "combined_series = pd.concat([train_series, val_series])\n",
        "final_model = ARIMA(combined_series, order=best_order).fit()\n",
        "test_forecast = final_model.forecast(steps=len(test_series))\n",
        "test_forecast.index = test_series.index\n",
        "test_metrics = compute_metrics(test_series.values, test_forecast.values)\n",
        "print(f\"Test hourly MAPE: {test_metrics['MAPE']:.2f}%\")\n",
        "val_daily_actual = val_series.resample(\"D\").mean()\n",
        "val_daily_pred = val_forecast.resample(\"D\").mean()\n",
        "val_daily_metrics = compute_metrics(val_daily_actual.values, val_daily_pred.values)\n",
        "test_daily_actual = test_series.resample(\"D\").mean()\n",
        "test_daily_pred = test_forecast.resample(\"D\").mean()\n",
        "test_daily_metrics = compute_metrics(test_daily_actual.values, test_daily_pred.values)\n",
        "hourly_results = pd.DataFrame({\"actual\": test_series.values, \"predicted\": test_forecast.values}, index=test_series.index)\n",
        "hourly_results.to_csv(\"arima_hourly_predictions.csv\")\n",
        "daily_results = pd.DataFrame({\"actual\": test_daily_actual.values, \"predicted\": test_daily_pred.values}, index=test_daily_actual.index)\n",
        "daily_results.to_csv(\"arima_daily_predictions.csv\")\n",
        "metrics_records = [\n",
        "    {\"Model\": \"ARIMA\", \"Dataset\": \"Validation\", **val_metrics},\n",
        "    {\"Model\": \"ARIMA\", \"Dataset\": \"Test\", **test_metrics},\n",
        "    {\"Model\": \"ARIMA\", \"Dataset\": \"Validation_Daily\", **val_daily_metrics},\n",
        "    {\"Model\": \"ARIMA\", \"Dataset\": \"Test_Daily\", **test_daily_metrics},\n",
        "]\n",
        "metrics_df = pd.DataFrame(metrics_records)\n",
        "metrics_df.to_csv(\"arima_metrics.csv\", index=False)\n",
        "print(\"Saved ARIMA outputs (predictions + metrics).\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "axes[0].plot(test_series.index, test_series.values, label=\"Actual\", linewidth=1.2)\n",
        "axes[0].plot(test_forecast.index, test_forecast.values, label=\"ARIMA Forecast\", linewidth=1.2)\n",
        "axes[0].set_title(\"ARIMA \u2013 Hourly Test Forecast\")\n",
        "axes[0].set_ylabel(\"Load (kW)\")\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].legend()\n",
        "\n",
        "residuals = test_series.values - test_forecast.values\n",
        "axes[1].hist(residuals, bins=30, color=\"#1f77b4\", edgecolor=\"black\", alpha=0.8)\n",
        "axes[1].axvline(0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
        "axes[1].set_title(\"ARIMA Residual Distribution\")\n",
        "axes[1].set_xlabel(\"Residual (kW)\")\n",
        "axes[1].grid(alpha=0.3)\n",
        "plt.savefig(\"arima_results.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      "outputs": []
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "np.random.seed(42)\n",
        "X_train = np.load(\"X_train_standard.npy\")\n",
        "X_val = np.load(\"X_val_standard.npy\")\n",
        "X_test = np.load(\"X_test_standard.npy\")\n",
        "y_train = np.load(\"y_train_standard.npy\")\n",
        "y_val = np.load(\"y_val_standard.npy\")\n",
        "y_test = np.load(\"y_test_standard.npy\")\n",
        "with open(\"y_scaler_standard.pkl\", \"rb\") as f:\n",
        "    y_scaler = pickle.load(f)\n",
        "feature_names = Path(\"feature_names.txt\").read_text().splitlines()\n",
        "train_index = pd.read_csv(\"train_data.csv\", index_col=\"datetime\", parse_dates=True).index\n",
        "val_index = pd.read_csv(\"val_data.csv\", index_col=\"datetime\", parse_dates=True).index\n",
        "test_df = pd.read_csv(\"test_data.csv\", index_col=\"datetime\", parse_dates=True)\n",
        "test_index = test_df.index\n",
        "test_actual = test_df[\"load_kw\"].values\n",
        "def inverse_transform(y_scaled: np.ndarray) -> np.ndarray:\n",
        "    return y_scaler.inverse_transform(y_scaled.reshape(-1, 1)).ravel()\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-3, None))) * 100\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
        "    return {\"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape, \"R2\": r2}\n",
        "candidate_configs = [\n",
        "    {\"hidden_layer_sizes\": (128, 64), \"alpha\": 1e-3},\n",
        "    {\"hidden_layer_sizes\": (96, 48), \"alpha\": 5e-4},\n",
        "    {\"hidden_layer_sizes\": (128,), \"alpha\": 5e-4},\n",
        "best_model = None\n",
        "best_val_mape = np.inf\n",
        "\n",
        "for cfg in candidate_configs:\n",
        "    model = MLPRegressor(\n",
        "        hidden_layer_sizes=cfg[\"hidden_layer_sizes\"],\n",
        "        alpha=cfg[\"alpha\"],\n",
        "        activation=\"relu\",\n",
        "        solver=\"adam\",\n",
        "        learning_rate=\"adaptive\",\n",
        "        max_iter=400,\n",
        "        verbose=False,\n",
        "    model.fit(X_train, y_train)\n",
        "    val_pred = inverse_transform(model.predict(X_val))\n",
        "    val_true = inverse_transform(y_val)\n",
        "    metrics = compute_metrics(val_true, val_pred)\n",
        "    if metrics[\"MAPE\"] < best_val_mape:\n",
        "        best_val_mape = metrics[\"MAPE\"]\n",
        "        best_model = model\n",
        "        best_config = cfg\n",
        "if best_model is None:\n",
        "    raise RuntimeError(\"MLP training failed for all configurations.\")\n",
        "print(f\"Selected MLP configuration: {best_config['hidden_layer_sizes']} (alpha={best_config['alpha']})\")\n",
        "train_pred = inverse_transform(best_model.predict(X_train))\n",
        "train_true = inverse_transform(y_train)\n",
        "train_metrics = compute_metrics(train_true, train_pred)\n",
        "val_pred = inverse_transform(best_model.predict(X_val))\n",
        "val_true = inverse_transform(y_val)\n",
        "val_metrics = compute_metrics(val_true, val_pred)\n",
        "print(f\"Validation hourly MAPE: {val_metrics['MAPE']:.2f}%\")\n",
        "X_train_val = np.vstack([X_train, X_val])\n",
        "y_train_val = np.concatenate([y_train, y_val])\n",
        "    hidden_layer_sizes=best_config[\"hidden_layer_sizes\"],\n",
        "    alpha=best_config[\"alpha\"],\n",
        "    activation=\"relu\",\n",
        "    solver=\"adam\",\n",
        "    learning_rate=\"adaptive\",\n",
        "    verbose=False,\n",
        "final_mlp.fit(X_train_val, y_train_val)\n",
        "test_pred = inverse_transform(final_mlp.predict(X_test))\n",
        "test_metrics = compute_metrics(test_actual, test_pred)\n",
        "print(f\"Test hourly MAPE: {test_metrics['MAPE']:.2f}%\")\n",
        "val_daily_actual = pd.Series(val_true, index=val_index).resample(\"D\").mean()\n",
        "val_daily_pred = pd.Series(val_pred, index=val_index).resample(\"D\").mean()\n",
        "val_daily_metrics = compute_metrics(val_daily_actual.values, val_daily_pred.values)\n",
        "\n",
        "test_daily_actual = pd.Series(test_actual, index=test_index).resample(\"D\").mean()\n",
        "test_daily_pred = pd.Series(test_pred, index=test_index).resample(\"D\").mean()\n",
        "test_daily_metrics = compute_metrics(test_daily_actual.values, test_daily_pred.values)\n",
        "\n",
        "hourly_results = pd.DataFrame({\"actual\": test_actual, \"predicted\": test_pred}, index=test_index)\n",
        "hourly_results.to_csv(\"mlp_hourly_predictions.csv\")\n",
        "\n",
        "daily_results = pd.DataFrame({\"actual\": test_daily_actual.values, \"predicted\": test_daily_pred.values}, index=test_daily_actual.index)\n",
        "daily_results.to_csv(\"mlp_daily_predictions.csv\")\n",
        "\n",
        "metrics_records = [\n",
        "    {\"Model\": \"MLP\", \"Dataset\": \"Train\", **train_metrics},\n",
        "    {\"Model\": \"MLP\", \"Dataset\": \"Validation\", **val_metrics},\n",
        "    {\"Model\": \"MLP\", \"Dataset\": \"Test\", **test_metrics},\n",
        "    {\"Model\": \"MLP\", \"Dataset\": \"Validation_Daily\", **val_daily_metrics},\n",
        "    {\"Model\": \"MLP\", \"Dataset\": \"Test_Daily\", **test_daily_metrics},\n",
        "]\n",
        "metrics_df = pd.DataFrame(metrics_records)\n",
        "metrics_df.to_csv(\"mlp_metrics.csv\", index=False)\n",
        "print(\"Saved MLP outputs (predictions + metrics).\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "axes[0].plot(test_index, test_actual, label=\"Actual\", linewidth=1.2)\n",
        "axes[0].plot(test_index, test_pred, label=\"MLP Forecast\", linewidth=1.2)\n",
        "axes[0].set_title(\"MLP \u2013 Hourly Test Forecast\")\n",
        "axes[0].set_ylabel(\"Load (kW)\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "residuals = test_actual - test_pred\n",
        "axes[1].hist(residuals, bins=30, color=\"#1f77b4\", edgecolor=\"black\", alpha=0.8)\n",
        "axes[1].axvline(0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
        "axes[1].set_title(\"MLP Residual Distribution\")\n",
        "axes[1].set_xlabel(\"Residual (kW)\")\n",
        "axes[1].grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mlp_results.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      "outputs": []
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "np.random.seed(42)\n",
        "with open(\"y_scaler_minmax.pkl\", \"rb\") as f:\n",
        "    y_scaler = pickle.load(f)\n",
        "\n",
        "X_train_all = np.load(\"X_train_minmax.npy\")\n",
        "X_val_all = np.load(\"X_val_minmax.npy\")\n",
        "X_test_all = np.load(\"X_test_minmax.npy\")\n",
        "y_train_all = np.load(\"y_train_minmax.npy\")\n",
        "y_val_all = np.load(\"y_val_minmax.npy\")\n",
        "y_test_all = np.load(\"y_test_minmax.npy\")\n",
        "\n",
        "feature_names = Path(\"feature_names.txt\").read_text().splitlines()\n",
        "\n",
        "selected_features = [\n",
        "    \"load_lag_1h\",\n",
        "    \"load_lag_24h\",\n",
        "    \"load_rolling_mean_24h\",\n",
        "    \"hour_sin\",\n",
        "    \"hour_cos\",\n",
        "    \"temperature_f\",\n",
        "    \"temp_humidity_index\",\n",
        "]\n",
        "feature_indices = [feature_names.index(feat) for feat in selected_features]\n",
        "\n",
        "X_train = X_train_all[:, feature_indices]\n",
        "X_val = X_val_all[:, feature_indices]\n",
        "X_test = X_test_all[:, feature_indices]\n",
        "\n",
        "train_df = pd.read_csv(\"train_data.csv\", index_col=\"datetime\", parse_dates=True)\n",
        "val_df = pd.read_csv(\"val_data.csv\", index_col=\"datetime\", parse_dates=True)\n",
        "test_df = pd.read_csv(\"test_data.csv\", index_col=\"datetime\", parse_dates=True)\n",
        "\n",
        "train_index = train_df.index\n",
        "val_index = val_df.index\n",
        "test_index = test_df.index\n",
        "\n",
        "train_actual = train_df[\"load_kw\"].values\n",
        "val_actual = val_df[\"load_kw\"].values\n",
        "test_actual = test_df[\"load_kw\"].values\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
        "    y_true = y_true[mask]\n",
        "    y_pred = y_pred[mask]\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-3, None))) * 100\n",
        "    ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n",
        "    r2 = 1 - np.sum((y_true - y_pred) ** 2) / ss_tot if ss_tot > 0 else np.nan\n",
        "    return {\"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape, \"R2\": r2}\n",
        "    def __init__(self, n_inputs, n_mfs=2, learning_rate=0.01, random_state=42):\n",
        "        self.n_rules = n_mfs ** n_inputs\n",
        "        self.random_state = np.random.RandomState(random_state)\n",
        "        self._rule_combos = list(np.ndindex(*[n_mfs] * n_inputs))\n",
        "        self.best_params = None\n",
        "        self._initialized = False\n",
        "        self.premise_params = self.random_state.normal(0, 0.1, size=(self.n_inputs, self.n_mfs, 2))\n",
        "        self.premise_params[:, :, 1] = np.abs(self.premise_params[:, :, 1]) + 0.1\n",
        "        self.consequent_params = self.random_state.normal(0, 0.1, size=(self.n_rules, self.n_inputs + 1))\n",
        "    def _initialize_with_data(self, X):\n",
        "            quantiles = np.linspace(0, 1, self.n_mfs + 2)[1:-1]\n",
        "            centers = np.quantile(X[:, i], quantiles)\n",
        "            spread = (np.quantile(X[:, i], 0.95) - np.quantile(X[:, i], 0.05)) / max(self.n_mfs, 1)\n",
        "            spread = float(max(spread, 0.05))\n",
        "            self.premise_params[i, :, 0] = centers\n",
        "            self.premise_params[i, :, 1] = spread\n",
        "        self._initialized = True\n",
        "        std = np.abs(std) + 1e-6\n",
        "        return np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
        "                membership[:, i, j] = self._gaussian_mf(X[:, i], self.premise_params[i, j, 0], self.premise_params[i, j, 1])\n",
        "        firing = np.ones((n_samples, self.n_rules))\n",
        "        for idx, combo in enumerate(self._rule_combos):\n",
        "                firing[:, idx] *= membership[:, input_idx, mf_idx]\n",
        "        return firing\n",
        "        sums = np.sum(firing_strengths, axis=1, keepdims=True) + 1e-8\n",
        "        return firing_strengths / sums\n",
        "            consequents[:, i] = X_bias @ self.consequent_params[i]\n",
        "        return normalized_strengths * consequents\n",
        "        normalized = self._normalization(firing_strengths)\n",
        "        weighted = self._consequent(X, normalized)\n",
        "        output = np.sum(weighted, axis=1)\n",
        "            \"membership\": membership,\n",
        "            \"normalized_strengths\": normalized,\n",
        "            \"X\": X,\n",
        "    @staticmethod\n",
        "    def _compute_loss(y_true, y_pred):\n",
        "    def _update_consequents(self, X, y, normalized_strengths):\n",
        "        design = np.zeros((n_samples, self.n_rules * (self.n_inputs + 1)))\n",
        "        for rule_idx in range(self.n_rules):\n",
        "            start = rule_idx * (self.n_inputs + 1)\n",
        "            design[:, start:start + self.n_inputs + 1] = normalized_strengths[:, [rule_idx]] * X_bias\n",
        "        theta, _, _, _ = np.linalg.lstsq(design, y, rcond=None)\n",
        "        self.consequent_params = theta.reshape(self.n_rules, self.n_inputs + 1)\n",
        "\n",
        "    def _update_premise_params(self, cache, y_true, output):\n",
        "        membership = cache[\"membership\"]\n",
        "        normalized = cache[\"normalized_strengths\"]\n",
        "        X = cache[\"X\"]\n",
        "        error = output - y_true\n",
        "        X_bias = np.hstack([X, np.ones((n_samples, 1))])\n",
        "                grad_mean = 0.0\n",
        "                grad_std = 0.0\n",
        "                for combo in self._rule_combos:\n",
        "                        std = np.abs(self.premise_params[i, j, 1]) + 1e-6\n",
        "                        rule_output = X_bias @ self.consequent_params[rule_idx]\n",
        "                        grad_component = 2 * error * normalized[:, rule_idx] * rule_output\n",
        "                        grad_mean += np.sum(grad_component * dmf_dmean) / n_samples\n",
        "                        grad_std += np.sum(grad_component * dmf_dstd) / n_samples\n",
        "                self.premise_params[i, j, 1] -= self.learning_rate * grad_std * 0.5\n",
        "                self.premise_params[i, j, 1] = np.abs(self.premise_params[i, j, 1]) + 1e-6\n",
        "    def fit(self, X_train, y_train, X_val=None, y_val=None, epochs=80, patience=12, verbose=False):\n",
        "        if not self._initialized:\n",
        "            self._initialize_with_data(X_train)\n",
        "        best_val = np.inf\n",
        "        patience_counter = 0\n",
        "            output, cache = self.forward(X_train)\n",
        "            loss = self._compute_loss(y_train, output)\n",
        "            self.train_losses.append(loss)\n",
        "            self._update_consequents(X_train, y_train, cache[\"normalized_strengths\"])\n",
        "            output, cache = self.forward(X_train)\n",
        "            self._update_premise_params(cache, y_train, output)\n",
        "                if val_loss < best_val - 1e-6:\n",
        "                    best_val = val_loss\n",
        "                        \"premise\": self.premise_params.copy(),\n",
        "                        \"consequent\": self.consequent_params.copy(),\n",
        "                val_loss = None\n",
        "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
        "                msg = f\"Epoch {epoch:03d} \u2192 loss {loss:.6f}\"\n",
        "                if val_loss is not None:\n",
        "                    msg += f\" | val {val_loss:.6f}\"\n",
        "                print(msg)\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "        if X_val is not None and y_val is not None and self.best_params is not None:\n",
        "            self.premise_params = self.best_params[\"premise\"]\n",
        "            self.consequent_params = self.best_params[\"consequent\"]\n",
        "\n",
        "    def extract_rules(self, feature_labels, X_reference):\n",
        "        membership = self._fuzzification(X_reference)\n",
        "        firing = self._rule_firing(membership)\n",
        "        normalized = self._normalization(firing)\n",
        "        avg_activation = normalized.mean(axis=0)\n",
        "        labels = [\"Low\", \"High\"] if self.n_mfs == 2 else [\"Low\", \"Medium\", \"High\"]\n",
        "        for idx, combo in enumerate(self._rule_combos):\n",
        "            antecedent = \" AND \".join(\n",
        "                f\"({feature_labels[i]} is {labels[combo[i]]})\" for i in range(self.n_inputs)\n",
        "            )\n",
        "            params = self.consequent_params[idx]\n",
        "            consequent_terms = \" + \".join(\n",
        "                f\"{params[j]:.3f}\u00b7{feature_labels[j]}\" for j in range(self.n_inputs)\n",
        "            )\n",
        "            consequent_expr = f\"{consequent_terms} + {params[-1]:.3f}\"\n",
        "            membership_params = \"; \".join(\n",
        "                f\"{feature_labels[i]}:{labels[combo[i]]}(\u03bc={self.premise_params[i, combo[i], 0]:.3f},\u03c3={abs(self.premise_params[i, combo[i], 1]):.3f})\"\n",
        "                for i in range(self.n_inputs)\n",
        "            )\n",
        "            rules.append(\n",
        "                {\n",
        "                    \"rule_id\": idx,\n",
        "                    \"antecedent\": antecedent,\n",
        "                    \"consequent\": consequent_expr,\n",
        "                    \"weight\": avg_activation[idx],\n",
        "                    \"membership_params\": membership_params,\n",
        "                }\n",
        "            )\n",
        "        rules.sort(key=lambda r: r[\"weight\"], reverse=True)\n",
        "        return rules\n",
        "base_anfis = ANFIS(n_inputs=X_train.shape[1], n_mfs=2, learning_rate=0.01, random_state=42)\n",
        "base_anfis.fit(X_train, y_train_all, X_val, y_val_all, epochs=90, patience=15, verbose=False)\n",
        "train_pred_scaled, _ = base_anfis.forward(X_train)\n",
        "val_pred_scaled, _ = base_anfis.forward(X_val)\n",
        "train_pred = y_scaler.inverse_transform(train_pred_scaled.reshape(-1, 1)).ravel()\n",
        "val_pred = y_scaler.inverse_transform(val_pred_scaled.reshape(-1, 1)).ravel()\n",
        "train_metrics = compute_metrics(train_actual, train_pred)\n",
        "val_metrics = compute_metrics(val_actual, val_pred)\n",
        "print(f\"Validation hourly MAPE: {val_metrics['MAPE']:.2f}%\")\n",
        "X_train_val = np.vstack([X_train, X_val])\n",
        "y_train_val = np.concatenate([y_train_all, y_val_all])\n",
        "final_anfis = ANFIS(n_inputs=X_train.shape[1], n_mfs=2, learning_rate=0.008, random_state=21)\n",
        "final_anfis.premise_params = base_anfis.premise_params.copy()\n",
        "final_anfis.consequent_params = base_anfis.consequent_params.copy()\n",
        "final_anfis._initialized = True\n",
        "final_anfis.fit(X_train_val, y_train_val, X_val=None, y_val=None, epochs=40, patience=0, verbose=False)\n",
        "test_pred_scaled, _ = final_anfis.forward(X_test)\n",
        "test_pred = y_scaler.inverse_transform(test_pred_scaled.reshape(-1, 1)).ravel()\n",
        "test_metrics = compute_metrics(test_actual, test_pred)\n",
        "print(f\"Test hourly MAPE: {test_metrics['MAPE']:.2f}%\")\n",
        "val_daily_actual = pd.Series(val_actual, index=val_index).resample(\"D\").mean()\n",
        "val_daily_pred = pd.Series(val_pred, index=val_index).resample(\"D\").mean()\n",
        "val_daily_metrics = compute_metrics(val_daily_actual.values, val_daily_pred.values)\n",
        "test_daily_actual = pd.Series(test_actual, index=test_index).resample(\"D\").mean()\n",
        "test_daily_pred = pd.Series(test_pred, index=test_index).resample(\"D\").mean()\n",
        "test_daily_metrics = compute_metrics(test_daily_actual.values, test_daily_pred.values)\n",
        "pd.DataFrame({\"actual\": test_actual, \"predicted\": test_pred}, index=test_index).to_csv(\"anfis_hourly_predictions.csv\")\n",
        "pd.DataFrame({\"actual\": test_daily_actual.values, \"predicted\": test_daily_pred.values}, index=test_daily_actual.index).to_csv(\"anfis_daily_predictions.csv\")\n",
        "metrics_records = [\n",
        "    {\"Model\": \"ANFIS\", \"Dataset\": \"Train\", **train_metrics},\n",
        "    {\"Model\": \"ANFIS\", \"Dataset\": \"Validation\", **val_metrics},\n",
        "    {\"Model\": \"ANFIS\", \"Dataset\": \"Test\", **test_metrics},\n",
        "    {\"Model\": \"ANFIS\", \"Dataset\": \"Validation_Daily\", **val_daily_metrics},\n",
        "    {\"Model\": \"ANFIS\", \"Dataset\": \"Test_Daily\", **test_daily_metrics},\n",
        "]\n",
        "metrics_df = pd.DataFrame(metrics_records)\n",
        "metrics_df.to_csv(\"anfis_metrics.csv\", index=False)\n",
        "print(\"Saved ANFIS outputs (predictions + metrics).\")\n",
        "rules = final_anfis.extract_rules(selected_features, X_train_val)\n",
        "rules_df.to_csv(\"anfis_fuzzy_rules.csv\", index=False)\n",
        "with open(\"anfis_fuzzy_rules.txt\", \"w\") as f:\n",
        "    for idx, rule in enumerate(rules, 1):\n",
        "        f.write(f\"Rule {idx}: IF {rule['antecedent']} THEN load = {rule['consequent']} | weight={rule['weight']:.4f}\\n\")\n",
        "\n",
        "print(\"Top ANFIS rules:\")\n",
        "for rule in rules[:10]:\n",
        "    print(f\"- IF {rule['antecedent']} THEN load = {rule['consequent']} (weight={rule['weight']:.3f})\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "axes[0].plot(test_index, test_actual, label=\"Actual\", linewidth=1.2)\n",
        "axes[0].plot(test_index, test_pred, label=\"ANFIS Forecast\", linewidth=1.2)\n",
        "axes[0].set_title(\"ANFIS \u2013 Hourly Test Forecast\")\n",
        "axes[0].set_ylabel(\"Load (kW)\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "residuals = test_actual - test_pred\n",
        "axes[1].hist(residuals, bins=30, color=\"#1f77b4\", edgecolor=\"black\", alpha=0.8)\n",
        "axes[1].axvline(0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
        "axes[1].set_title(\"ANFIS Residual Distribution\")\n",
        "axes[1].set_xlabel(\"Residual (kW)\")\n",
        "axes[1].grid(alpha=0.3)\n",
        "plt.savefig(\"anfis_results.png\", dpi=300, bbox_inches=\"tight\")\n",
        "metrics_map = {\n",
        "    \"ARIMA\": pd.read_csv(\"arima_metrics.csv\"),\n",
        "    \"MLP\": pd.read_csv(\"mlp_metrics.csv\"),\n",
        "    \"ANFIS\": metrics_df,\n",
        "}\n",
        "def lookup_metric(df, dataset, metric):\n",
        "    row = df[df[\"Dataset\"] == dataset]\n",
        "    return float(row[metric]) if not row.empty else np.nan\n",
        "\n",
        "summary_columns = [\n",
        "    (\"Hourly Validation\", \"RMSE\"),\n",
        "    (\"Hourly Validation\", \"MAE\"),\n",
        "    (\"Hourly Validation\", \"MAPE\"),\n",
        "    (\"Hourly Validation\", \"R2\"),\n",
        "    (\"Hourly Test\", \"RMSE\"),\n",
        "    (\"Hourly Test\", \"MAE\"),\n",
        "    (\"Hourly Test\", \"MAPE\"),\n",
        "    (\"Hourly Test\", \"R2\"),\n",
        "    (\"Day-Ahead Validation\", \"RMSE\"),\n",
        "    (\"Day-Ahead Validation\", \"MAE\"),\n",
        "    (\"Day-Ahead Validation\", \"MAPE\"),\n",
        "    (\"Day-Ahead Test\", \"RMSE\"),\n",
        "    (\"Day-Ahead Test\", \"MAE\"),\n",
        "    (\"Day-Ahead Test\", \"MAPE\"),\n",
        "]\n",
        "summary_index = [\"ARIMA\", \"MLP\", \"ANFIS\"]\n",
        "summary = pd.DataFrame(index=summary_index, columns=pd.MultiIndex.from_tuples(summary_columns), dtype=float)\n",
        "\n",
        "for model_name in summary_index:\n",
        "    df_metrics = metrics_map[model_name]\n",
        "    summary.loc[model_name, (\"Hourly Validation\", \"RMSE\")] = lookup_metric(df_metrics, \"Validation\", \"RMSE\")\n",
        "    summary.loc[model_name, (\"Hourly Validation\", \"MAE\")] = lookup_metric(df_metrics, \"Validation\", \"MAE\")\n",
        "    summary.loc[model_name, (\"Hourly Validation\", \"MAPE\")] = lookup_metric(df_metrics, \"Validation\", \"MAPE\")\n",
        "    summary.loc[model_name, (\"Hourly Validation\", \"R2\")] = lookup_metric(df_metrics, \"Validation\", \"R2\")\n",
        "    summary.loc[model_name, (\"Hourly Test\", \"RMSE\")] = lookup_metric(df_metrics, \"Test\", \"RMSE\")\n",
        "    summary.loc[model_name, (\"Hourly Test\", \"MAE\")] = lookup_metric(df_metrics, \"Test\", \"MAE\")\n",
        "    summary.loc[model_name, (\"Hourly Test\", \"MAPE\")] = lookup_metric(df_metrics, \"Test\", \"MAPE\")\n",
        "    summary.loc[model_name, (\"Hourly Test\", \"R2\")] = lookup_metric(df_metrics, \"Test\", \"R2\")\n",
        "    summary.loc[model_name, (\"Day-Ahead Validation\", \"RMSE\")] = lookup_metric(df_metrics, \"Validation_Daily\", \"RMSE\")\n",
        "    summary.loc[model_name, (\"Day-Ahead Validation\", \"MAE\")] = lookup_metric(df_metrics, \"Validation_Daily\", \"MAE\")\n",
        "    summary.loc[model_name, (\"Day-Ahead Validation\", \"MAPE\")] = lookup_metric(df_metrics, \"Validation_Daily\", \"MAPE\")\n",
        "    summary.loc[model_name, (\"Day-Ahead Test\", \"RMSE\")] = lookup_metric(df_metrics, \"Test_Daily\", \"RMSE\")\n",
        "    summary.loc[model_name, (\"Day-Ahead Test\", \"MAE\")] = lookup_metric(df_metrics, \"Test_Daily\", \"MAE\")\n",
        "    summary.loc[model_name, (\"Day-Ahead Test\", \"MAPE\")] = lookup_metric(df_metrics, \"Test_Daily\", \"MAPE\")\n",
        "\n",
        "ranking_columns = pd.MultiIndex.from_tuples([\n",
        "    (\"Ranking\", \"Best Hourly\"),\n",
        "    (\"Ranking\", \"Best Day-Ahead\"),\n",
        "])\n",
        "for col in ranking_columns:\n",
        "    summary[col] = \"\"\n",
        "\n",
        "hourly_mape = summary[(\"Hourly Test\", \"MAPE\")].astype(float)\n",
        "min_hourly_mape = hourly_mape.min()\n",
        "hourly_candidates = summary[hourly_mape == min_hourly_mape]\n",
        "best_hourly = hourly_candidates[(\"Hourly Test\", \"RMSE\")].astype(float).idxmin()\n",
        "\n",
        "aday_mape = summary[(\"Day-Ahead Test\", \"MAPE\")].astype(float)\n",
        "min_day_mape = aday_mape.min()\n",
        "day_candidates = summary[aday_mape == min_day_mape]\n",
        "best_day = day_candidates[(\"Day-Ahead Test\", \"RMSE\")].astype(float).idxmin()\n",
        "\n",
        "summary.loc[best_hourly, (\"Ranking\", \"Best Hourly\")] = \"\u2605\"\n",
        "summary.loc[best_day, (\"Ranking\", \"Best Day-Ahead\")] = \"\u2605\"\n",
        "\n",
        "ordered_columns = list(pd.MultiIndex.from_tuples(summary_columns)) + list(ranking_columns)\n",
        "summary = summary[ordered_columns]\n",
        "summary.to_csv(\"model_comparison_summary.csv\")\n",
        "\n",
        "print(\"Model comparison summary (lower is better):\")\n",
        "print(summary.round(3))\n",
        "\n",
        "print(\"\\nANFIS test hourly metrics:\")\n",
        "for key in [\"RMSE\", \"MAE\", \"MAPE\", \"R2\"]:\n",
        "    print(f\"  {key}: {test_metrics[key]:.3f}\")\n",
        "\n",
        "print(\"\\nANFIS test day-ahead metrics:\")\n",
        "for key in [\"RMSE\", \"MAE\", \"MAPE\"]:\n",
        "    print(f\"  {key}: {test_daily_metrics[key]:.3f}\")\n"
      "outputs": []