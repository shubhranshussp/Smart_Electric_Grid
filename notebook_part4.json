{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Smart Electric Grid - Energy Forecasting (OPTIMIZED)**\n\n**Models:** ARIMA, MLP (Optimized), ANFIS (Enhanced)\n\n**Key Optimizations:**\n- PCA feature reduction (26 \u2192 12-15 features)\n- Improved MLP with deeper architecture\n- Enhanced ANFIS with <3% MAPE target\n- Unified metrics and comparison\n- Cyclical encoding for temporal features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Global Imports and Configuration**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n# GLOBAL IMPORTS - All libraries in one place (eliminates repetition)\n# =============================================================================\n\n# Core libraries\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Machine Learning - Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Machine Learning - Models\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Statistical models\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Utilities\nimport pickle\nfrom datetime import datetime, timedelta\nfrom sklearn.cluster import KMeans\nimport itertools\nimport time\n\n# =============================================================================\n# GLOBAL CONFIGURATION\n# =============================================================================\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\n# Data split ratios\nTRAIN_RATIO = 0.70\nVAL_RATIO = 0.10\nTEST_RATIO = 0.20\n\n# PCA configuration\nN_PCA_COMPONENTS = 12  # Reduced from 26 features\n\n# Model configurations\nMLP_CONFIG = {\n    'max_iter': 1000,\n    'early_stopping': True,\n    'validation_fraction': 0.1,\n    'n_iter_no_change': 30,\n    'random_state': RANDOM_SEED\n}\n\nANFIS_CONFIG = {\n    'n_membership_functions': 2,  # Reduced for efficiency\n    'max_epochs': 300,  # Increased for better training\n    'learning_rate': 0.01,\n    'patience': 50\n}\n\nprint(\"\u2713 All imports loaded successfully\")\nprint(f\"\u2713 Random seed set to: {RANDOM_SEED}\")\nprint(f\"\u2713 PCA components: {N_PCA_COMPONENTS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Utility Functions (Unified Metrics & Plotting)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n# UNIFIED METRIC CALCULATION FUNCTION (eliminates duplication across models)\n# =============================================================================\n\ndef calculate_metrics(y_true, y_pred, model_name=\"Model\"):\n    \"\"\"\n    Calculate comprehensive metrics for model evaluation\n\n    Parameters:\n    -----------\n    y_true : array-like\n        Actual values\n    y_pred : array-like\n        Predicted values\n    model_name : str\n        Name of the model for display\n\n    Returns:\n    --------\n    dict : Dictionary containing all metrics\n    \"\"\"\n    # Ensure arrays\n    y_true = np.array(y_true).flatten()\n    y_pred = np.array(y_pred).flatten()\n\n    # Calculate metrics\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n\n    # MAPE with safe division\n    mask = y_true != 0\n    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n    mape = np.clip(mape, 0, 100)  # Clip to reasonable range\n\n    # R-squared\n    r2 = r2_score(y_true, y_pred)\n\n    # Print results\n    print(f\"\\n{'='*60}\")\n    print(f\"{model_name} - Performance Metrics\")\n    print(f\"{'='*60}\")\n    print(f\"RMSE (Root Mean Squared Error): {rmse:.2f} kW\")\n    print(f\"MAE  (Mean Absolute Error):     {mae:.2f} kW\")\n    print(f\"MAPE (Mean Absolute % Error):   {mape:.2f} %\")\n    print(f\"R\u00b2   (R-squared Score):         {r2:.4f}\")\n    print(f\"{'='*60}\")\n\n    return {\n        'RMSE': rmse,\n        'MAE': mae,\n        'MAPE': mape,\n        'R2': r2\n    }\n\n# =============================================================================\n# UNIFIED PLOTTING FUNCTION\n# =============================================================================\n\ndef plot_predictions(y_true, y_pred, model_name=\"Model\", save_path=None):\n    \"\"\"Create comprehensive prediction visualization\"\"\"\n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n    # 1. Actual vs Predicted (Scatter)\n    axes[0, 0].scatter(y_true, y_pred, alpha=0.5)\n    axes[0, 0].plot([y_true.min(), y_true.max()],\n                     [y_true.min(), y_true.max()], 'r--', lw=2)\n    axes[0, 0].set_xlabel('Actual Load (kW)')\n    axes[0, 0].set_ylabel('Predicted Load (kW)')\n    axes[0, 0].set_title(f'{model_name}: Actual vs Predicted')\n    axes[0, 0].grid(True)\n\n    # 2. Time series comparison (first 200 points)\n    n_points = min(200, len(y_true))\n    axes[0, 1].plot(y_true[:n_points], label='Actual', linewidth=2)\n    axes[0, 1].plot(y_pred[:n_points], label='Predicted', linewidth=2, alpha=0.7)\n    axes[0, 1].set_xlabel('Time Index')\n    axes[0, 1].set_ylabel('Load (kW)')\n    axes[0, 1].set_title(f'{model_name}: Time Series Comparison')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True)\n\n    # 3. Residuals\n    residuals = y_true - y_pred\n    axes[1, 0].scatter(range(len(residuals)), residuals, alpha=0.5)\n    axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n    axes[1, 0].set_xlabel('Sample Index')\n    axes[1, 0].set_ylabel('Residual (kW)')\n    axes[1, 0].set_title(f'{model_name}: Residuals Plot')\n    axes[1, 0].grid(True)\n\n    # 4. Residual distribution\n    axes[1, 1].hist(residuals, bins=50, edgecolor='black')\n    axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n    axes[1, 1].set_xlabel('Residual (kW)')\n    axes[1, 1].set_ylabel('Frequency')\n    axes[1, 1].set_title(f'{model_name}: Residual Distribution')\n    axes[1, 1].grid(True)\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n\nprint(\"\u2713 Utility functions defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Load Dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset (remove Google Colab dependency)\ntry:\n    from google.colab import files\n    uploaded = files.upload()\n    print(\"\u2713 File uploaded via Google Colab\")\nexcept:\n    print(\"\u2713 Running in local environment - Dataset.csv should be present\")\n\n# Load data\ndf = pd.read_csv('Dataset.csv')\nprint(f\"\\n\u2713 Dataset loaded successfully: {df.shape}\")\nprint(f\"  Rows: {df.shape[0]:,} | Columns: {df.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Data Cleaning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n# DATA CLEANING - Streamlined and optimized\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"DATA CLEANING PROCESS\")\nprint(\"=\"*70)\n\n# Step 1: Fix DATE column\ndf['DATE'] = df['DATE'].fillna(method='ffill')\nprint(f\"\u2713 Step 1: Fixed DATE column\")\n\n# Step 2: Select relevant columns\ncolumns_to_keep = [\n    'DATE', 'TIME', 'VOLTAGE', 'CURRENT', 'PF',\n    'POWER (KW)', '\"WEEKEND/WEEKDAY\"', 'SEASON',\n    'Temp (F)', 'Humidity (%)'\n]\ndf_clean = df[columns_to_keep].copy()\nprint(f\"\u2713 Step 2: Kept {len(columns_to_keep)} relevant columns\")\n\n# Step 3: Rename columns\ncolumn_mapping = {\n    'DATE': 'date',\n    'TIME': 'time',\n    'VOLTAGE': 'voltage',\n    'CURRENT': 'current',\n    'PF': 'power_factor',\n    'POWER (KW)': 'load_kw',\n    '\"WEEKEND/WEEKDAY\"': 'is_weekend',\n    'SEASON': 'season',\n    'Temp (F)': 'temperature_f',\n    'Humidity (%)': 'humidity'\n}\ndf_clean.rename(columns=column_mapping, inplace=True)\nprint(f\"\u2713 Step 3: Renamed columns for consistency\")\n\n# Step 4: Create datetime index\ndf_clean['time'] = df_clean['time'].str.replace('-', ':')\ndf_clean['datetime'] = pd.to_datetime(\n    df_clean['date'] + ' ' + df_clean['time'],\n    format='%d/%m/%Y %H:%M',\n    errors='coerce'\n)\ndf_clean.set_index('datetime', inplace=True)\ndf_clean.drop(['date', 'time'], axis=1, inplace=True)\nprint(f\"\u2713 Step 4: Created datetime index\")\n\n# Step 5: Handle missing values\ndf_clean = df_clean.fillna(method='ffill').fillna(method='bfill')\nprint(f\"\u2713 Step 5: Handled missing values\")\n\n# Step 6: Convert is_weekend to binary\ndf_clean['is_weekend'] = df_clean['is_weekend'].apply(\n    lambda x: 1 if str(x).strip().upper() == 'WEEKEND' else 0\n)\nprint(f\"\u2713 Step 6: Converted is_weekend to binary (0/1)\")\n\nprint(f\"\\n\u2713 CLEANING COMPLETE\")\nprint(f\"  Final shape: {df_clean.shape}\")\nprint(f\"  Date range: {df_clean.index.min()} to {df_clean.index.max()}\")\nprint(f\"  Total hours: {len(df_clean)}\")\n\n# Cache cleaned data in memory (eliminates repeated file loading)\nCLEANED_DATA = df_clean.copy()\n\n# Save for backup\ndf_clean.to_csv('cleaned_electric_load_data.csv')\nprint(f\"\\n\u2713 Saved: cleaned_electric_load_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Feature Engineering (with PCA Optimization)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n# FEATURE ENGINEERING - Enhanced with PCA, Cyclical Encoding, One-Hot Encoding\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"FEATURE ENGINEERING PROCESS\")\nprint(\"=\"*70)\n\n# Start with cleaned data from memory (no file reload needed!)\ndf_features = CLEANED_DATA.copy()\n\n# Extract datetime features\ndf_features['hour'] = df_features.index.hour\ndf_features['day_of_week'] = df_features.index.dayofweek\ndf_features['month'] = df_features.index.month\n\nprint(f\"\u2713 Extracted basic datetime features\")\n\n# -----------------------------------------------\n# LAG FEATURES\n# -----------------------------------------------\ndf_features['load_lag_1h'] = df_features['load_kw'].shift(1)\ndf_features['load_lag_24h'] = df_features['load_kw'].shift(24)\ndf_features['load_lag_168h'] = df_features['load_kw'].shift(168)\nprint(f\"\u2713 Created lag features (1h, 24h, 168h)\")\n\n# -----------------------------------------------\n# ROLLING STATISTICS\n# -----------------------------------------------\ndf_features['load_rolling_mean_3h'] = df_features['load_kw'].rolling(window=3, min_periods=1).mean()\ndf_features['load_rolling_mean_6h'] = df_features['load_kw'].rolling(window=6, min_periods=1).mean()\ndf_features['load_rolling_mean_24h'] = df_features['load_kw'].rolling(window=24, min_periods=1).mean()\ndf_features['load_rolling_std_24h'] = df_features['load_kw'].rolling(window=24, min_periods=1).std()\nprint(f\"\u2713 Created rolling statistics features\")\n\n# -----------------------------------------------\n# CYCLICAL ENCODING (NEW! - Better hour representation)\n# -----------------------------------------------\ndf_features['hour_sin'] = np.sin(2 * np.pi * df_features['hour'] / 24)\ndf_features['hour_cos'] = np.cos(2 * np.pi * df_features['hour'] / 24)\ndf_features['day_of_week_sin'] = np.sin(2 * np.pi * df_features['day_of_week'] / 7)\ndf_features['day_of_week_cos'] = np.cos(2 * np.pi * df_features['day_of_week'] / 7)\ndf_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\ndf_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\nprint(f\"\u2713 Created cyclical encoding (sin/cos) for temporal features\")\n\n# -----------------------------------------------\n# TEMPORAL INDICATORS\n# -----------------------------------------------\ndf_features['is_peak_morning'] = ((df_features['hour'] >= 7) & (df_features['hour'] <= 9)).astype(int)\ndf_features['is_peak_evening'] = ((df_features['hour'] >= 18) & (df_features['hour'] <= 21)).astype(int)\ndf_features['is_peak_hour'] = (df_features['is_peak_morning'] | df_features['is_peak_evening']).astype(int)\ndf_features['is_night'] = ((df_features['hour'] >= 0) & (df_features['hour'] <= 5)).astype(int)\ndf_features['is_working_hours'] = ((df_features['hour'] >= 9) & (df_features['hour'] <= 17)).astype(int)\ndf_features['weekend_hour'] = df_features['is_weekend'] * df_features['hour']\nprint(f\"\u2713 Created temporal indicator features\")\n\n# -----------------------------------------------\n# WEATHER INTERACTIONS\n# -----------------------------------------------\ndf_features['temp_humidity_index'] = df_features['temperature_f'] * (df_features['humidity'] / 100)\ndf_features['temp_deviation'] = df_features['temperature_f'] - df_features['temperature_f'].mean()\ndf_features['season_temp'] = df_features['season'] * df_features['temperature_f']\nprint(f\"\u2713 Created weather interaction features\")\n\n# -----------------------------------------------\n# ONE-HOT ENCODING FOR SEASON (NEW! - Better categorical handling)\n# -----------------------------------------------\nseason_dummies = pd.get_dummies(df_features['season'], prefix='season')\ndf_features = pd.concat([df_features, season_dummies], axis=1)\nprint(f\"\u2713 Created one-hot encoding for season\")\n\n# Remove rows with NaN (from lag features)\ndf_features = df_features.dropna()\nprint(f\"\\n\u2713 Removed NaN rows: {len(df_features)} samples remaining\")\n\n# -----------------------------------------------\n# PREPARE FEATURES AND TARGET\n# -----------------------------------------------\n# Target variable\ny = df_features['load_kw'].values\n\n# Feature columns (before PCA)\nfeature_cols = [col for col in df_features.columns if col != 'load_kw']\nX_original = df_features[feature_cols].values\n\nprint(f\"\\n\u2713 Original feature set: {X_original.shape[1]} features\")\nprint(f\"  Total samples: {X_original.shape[0]}\")\n\n# -----------------------------------------------\n# TRAIN-VAL-TEST SPLIT (Temporal)\n# -----------------------------------------------\nn_total = len(X_original)\nn_train = int(n_total * TRAIN_RATIO)\nn_val = int(n_total * VAL_RATIO)\n\nX_train_orig = X_original[:n_train]\nX_val_orig = X_original[n_train:n_train+n_val]\nX_test_orig = X_original[n_train+n_val:]\n\ny_train = y[:n_train]\ny_val = y[n_train:n_train+n_val]\ny_test = y[n_train+n_val:]\n\nprint(f\"\\n\u2713 Data split:\")\nprint(f\"  Train: {len(X_train_orig)} samples ({TRAIN_RATIO*100:.0f}%)\")\nprint(f\"  Val:   {len(X_val_orig)} samples ({VAL_RATIO*100:.0f}%)\")\nprint(f\"  Test:  {len(X_test_orig)} samples ({TEST_RATIO*100:.0f}%)\")\n\n# -----------------------------------------------\n# FEATURE SCALING (fit on train, apply to all)\n# -----------------------------------------------\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train_orig)\nX_val_scaled = scaler_X.transform(X_val_orig)\nX_test_scaled = scaler_X.transform(X_test_orig)\n\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\ny_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).ravel()\ny_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n\nprint(f\"\\n\u2713 Feature scaling completed (StandardScaler)\")\n\n# -----------------------------------------------\n# PCA DIMENSIONALITY REDUCTION (KEY OPTIMIZATION!)\n# -----------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(f\"PCA DIMENSIONALITY REDUCTION\")\nprint(f\"{'='*70}\")\n\npca = PCA(n_components=N_PCA_COMPONENTS, random_state=RANDOM_SEED)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_val_pca = pca.transform(X_val_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance)\n\nprint(f\"\u2713 PCA reduced features: {X_original.shape[1]} \u2192 {N_PCA_COMPONENTS}\")\nprint(f\"\u2713 Total variance explained: {cumulative_variance[-1]*100:.2f}%\")\nprint(f\"\\nVariance by component:\")\nfor i, (var, cum_var) in enumerate(zip(explained_variance[:5], cumulative_variance[:5])):\n    print(f\"  PC{i+1}: {var*100:.2f}% (cumulative: {cum_var*100:.2f}%)\")\nprint(f\"  ...\")\n\n# -----------------------------------------------\n# SAVE PROCESSED DATA\n# -----------------------------------------------\n# Save PCA-transformed data (for MLP and ANFIS)\nnp.save('X_train_pca.npy', X_train_pca)\nnp.save('X_val_pca.npy', X_val_pca)\nnp.save('X_test_pca.npy', X_test_pca)\n\nnp.save('y_train.npy', y_train)\nnp.save('y_val.npy', y_val)\nnp.save('y_test.npy', y_test)\n\nnp.save('y_train_scaled.npy', y_train_scaled)\nnp.save('y_val_scaled.npy', y_val_scaled)\nnp.save('y_test_scaled.npy', y_test_scaled)\n\n# Save original scaled features (for ARIMA if needed)\nnp.save('X_train_scaled.npy', X_train_scaled)\nnp.save('X_val_scaled.npy', X_val_scaled)\nnp.save('X_test_scaled.npy', X_test_scaled)\n\n# Save scalers and PCA\nwith open('scaler_X.pkl', 'wb') as f:\n    pickle.dump(scaler_X, f)\nwith open('scaler_y.pkl', 'wb') as f:\n    pickle.dump(scaler_y, f)\nwith open('pca_model.pkl', 'wb') as f:\n    pickle.dump(pca, f)\n\nprint(f\"\\n\u2713 All processed data saved successfully\")\nprint(f\"\\n{'='*70}\")\nprint(f\"FEATURE ENGINEERING COMPLETE\")\nprint(f\"{'='*70}\")\nprint(f\"\u2713 Ready for model training with {N_PCA_COMPONENTS} PCA features\")\nprint(f\"\u2713 This will dramatically speed up ANFIS training!\")\nprint(f\"  (Rules reduced from 3^{X_original.shape[1]} to 3^{N_PCA_COMPONENTS})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **ARIMA Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n# ARIMA MODEL - TIME SERIES FORECASTING\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"ARIMA MODEL - AUTOREGRESSIVE INTEGRATED MOVING AVERAGE\")\nprint(\"=\"*70)\n\n# Load target data (use non-PCA data for ARIMA - it's univariate time series)\ny_train_arima = np.load('y_train.npy')\ny_val_arima = np.load('y_val.npy')\ny_test_arima = np.load('y_test.npy')\n\n# Create time series\ntrain_val_series = np.concatenate([y_train_arima, y_val_arima])\n\nprint(f\"\\n\u2713 Data loaded:\")\nprint(f\"  Train: {len(y_train_arima)} samples\")\nprint(f\"  Val:   {len(y_val_arima)} samples\")\nprint(f\"  Test:  {len(y_test_arima)} samples\")\n\n# -----------------------------------------------\n# STEP 1: Stationarity Check\n# -----------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(\"STEP 1: Stationarity Check (ADF Test)\")\nprint(f\"{'='*70}\")\n\nresult = adfuller(y_train_arima)\nprint(f\"\\nADF Statistic: {result[0]:.4f}\")\nprint(f\"p-value: {result[1]:.4f}\")\nif result[1] <= 0.05:\n    print(\"\u2713 Series is STATIONARY (p-value <= 0.05)\")\nelse:\n    print(\"\u2717 Series is NON-STATIONARY (p-value > 0.05)\")\n\n# -----------------------------------------------\n# STEP 2: ACF & PACF Analysis\n# -----------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(\"STEP 2: ACF & PACF Analysis\")\nprint(f\"{'='*70}\")\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\nplot_acf(y_train_arima, lags=48, ax=axes[0])\naxes[0].set_title('Autocorrelation Function (ACF)', fontsize=12, fontweight='bold')\nplot_pacf(y_train_arima, lags=48, ax=axes[1])\naxes[1].set_title('Partial Autocorrelation Function (PACF)', fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.savefig('arima_acf_pacf.png', dpi=150, bbox_inches='tight')\nprint(\"\u2713 Saved: arima_acf_pacf.png\")\nplt.show()\n\n# -----------------------------------------------\n# STEP 3: Model Selection (Quick)\n# -----------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(\"STEP 3: ARIMA Model Selection\")\nprint(f\"{'='*70}\")\n\nconfigs = [(1,0,1), (2,0,1), (1,0,2), (2,0,2)]\nbest_aic = np.inf\nbest_order = None\n\nprint(\"\\nTesting ARIMA configurations...\")\nfor order in configs:\n    try:\n        model = ARIMA(y_train_arima, order=order)\n        fitted = model.fit()\n        print(f\"  ARIMA{order}: AIC={fitted.aic:.2f}\")\n        if fitted.aic < best_aic:\n            best_aic = fitted.aic\n            best_order = order\n    except:\n        print(f\"  ARIMA{order}: Failed\")\n\nprint(f\"\\n\u2713 Best order: ARIMA{best_order} (AIC={best_aic:.2f})\")\n\n# -----------------------------------------------\n# STEP 4: Train Final Model\n# -----------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(\"STEP 4: Training Final ARIMA Model\")\nprint(f\"{'='*70}\")\n\nprint(f\"\\nTraining ARIMA{best_order} on train+val data...\")\nfinal_arima = ARIMA(train_val_series, order=best_order)\nfitted_arima = final_arima.fit()\nprint(\"\u2713 Model trained successfully\")\n\n# -----------------------------------------------\n# STEP 5: Predictions\n# -----------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(\"STEP 5: Generating Predictions\")\nprint(f\"{'='*70}\")\n\n# Forecast test samples\nforecast_result = fitted_arima.forecast(steps=len(y_test_arima))\ny_pred_arima = np.array(forecast_result)\n\nprint(f\"\u2713 Generated {len(y_pred_arima)} predictions\")\n\n# -----------------------------------------------\n# STEP 6: Evaluation\n# -----------------------------------------------\narima_metrics = calculate_metrics(y_test_arima, y_pred_arima, \"ARIMA\")\n\n# Save results\nnp.save('arima_predictions.npy', y_pred_arima)\nwith open('arima_model.pkl', 'wb') as f:\n    pickle.dump(fitted_arima, f)\n\n# Visualizations\nplot_predictions(y_test_arima, y_pred_arima, \"ARIMA\", \"arima_predictions.png\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"ARIMA MODEL COMPLETE\")\nprint(f\"{'='*70}\")\nprint(f\"\u2713 MAPE: {arima_metrics['MAPE']:.2f}%\")\nprint(f\"\u2713 Model saved: arima_model.pkl\")\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **MLP Model (Optimized)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n# MLP MODEL - MULTI-LAYER PERCEPTRON (OPTIMIZED WITH MODERN TECHNIQUES)\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"MLP MODEL - OPTIMIZED NEURAL NETWORK\")\nprint(\"=\"*70)\n\n# Load PCA-transformed data\nX_train_mlp = np.load('X_train_pca.npy')\nX_val_mlp = np.load('X_val_pca.npy')\nX_test_mlp = np.load('X_test_pca.npy')\n\ny_train_mlp = np.load('y_train_scaled.npy')\ny_val_mlp = np.load('y_val_scaled.npy')\ny_test_mlp = np.load('y_test_scaled.npy')\n\nprint(f\"\\n\u2713 Data loaded (PCA features):\")\nprint(f\"  Train: {X_train_mlp.shape}\")\nprint(f\"  Val:   {X_val_mlp.shape}\")\nprint(f\"  Test:  {X_test_mlp.shape}\")\n\n# -----------------------------------------------\n# STEP 1: Test Multiple Architectures\n# -----------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(\"STEP 1: Testing Multiple MLP Architectures\")\nprint(f\"{'='*70}\")\n\n# Define configurations to test (both 2-layer and 3-layer)\nmlp_configs = [\n    {\n        'name': '2-Layer (128, 64)',\n        'hidden_layer_sizes': (128, 64),\n        'alpha': 0.0001,\n        'learning_rate_init': 0.001\n    },\n    {\n        'name': '2-Layer (256, 128) - Wider',\n        'hidden_layer_sizes': (256, 128),\n        'alpha': 0.0001,\n        'learning_rate_init': 0.001\n    },\n    {\n        'name': '3-Layer (256, 128, 64) - Deep',\n        'hidden_layer_sizes': (256, 128, 64),\n        'alpha': 0.0001,\n        'learning_rate_init': 0.001\n    },\n    {\n        'name': '3-Layer (128, 64, 32)',\n        'hidden_layer_sizes': (128, 64, 32),\n        'alpha': 0.0001,\n        'learning_rate_init': 0.0005\n    },\n    {\n        'name': '2-Layer (128, 64) - High Reg',\n        'hidden_layer_sizes': (128, 64),\n        'alpha': 0.001,\n        'learning_rate_init': 0.001\n    }\n]\n\nprint(f\"\\nTesting {len(mlp_configs)} configurations...\\n\")\n\nbest_val_rmse = np.inf\nbest_config = None\nbest_model = None\n\nfor i, config in enumerate(mlp_configs, 1):\n    print(f\"[{i}/{len(mlp_configs)}] Testing: {config['name']}\")\n\n    # Create model with configuration\n    mlp = MLPRegressor(\n        hidden_layer_sizes=config['hidden_layer_sizes'],\n        activation='relu',\n        solver='adam',\n        alpha=config['alpha'],\n        batch_size=32,\n        learning_rate_init=config['learning_rate_init'],\n        max_iter=1000,\n        early_stopping=True,\n        validation_fraction=0.1,\n        n_iter_no_change=30,\n        random_state=RANDOM_SEED,\n        verbose=False\n    )\n\n    # Train\n    start_time = time.time()\n    mlp.fit(X_train_mlp, y_train_mlp)\n    train_time = time.time() - start_time\n\n    # Validate\n    y_val_pred = mlp.predict(X_val_mlp)\n    val_rmse = np.sqrt(mean_squared_error(y_val_mlp, y_val_pred))\n\n    print(f\"  Val RMSE: {val_rmse:.4f} | Training time: {train_time:.2f}s | Epochs: {mlp.n_iter_}\")\n\n    # Track best\n    if val_rmse < best_val_rmse:\n        best_val_rmse = val_rmse\n        best_config = config\n        best_model = mlp\n        print(f\"  \u2713 NEW BEST!\")\n    print()\n\nprint(f\"{'='*70}\")\nprint(f\"BEST CONFIGURATION: {best_config['name']}\")\nprint(f\"Validation RMSE: {best_val_rmse:.4f}\")\nprint(f\"{'='*70}\")\n\n# -----------------------------------------------\n# STEP 2: Retrain Best Model on Train+Val\n# -----------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(\"STEP 2: Retraining Best Model on Train+Val\")\nprint(f\"{'='*70}\")\n\n# Combine train and val\nX_train_val = np.concatenate([X_train_mlp, X_val_mlp])\ny_train_val = np.concatenate([y_train_mlp, y_val_mlp])\n\nprint(f\"\\nTraining {best_config['name']} on {len(X_train_val)} samples...\")\n\nfinal_mlp = MLPRegressor(\n    hidden_layer_sizes=best_config['hidden_layer_sizes'],\n    activation='relu',\n    solver='adam',\n    alpha=best_config['alpha'],\n    batch_size=32,\n    learning_rate_init=best_config['learning_rate_init'],\n    max_iter=1000,\n    early_stopping=True,\n    validation_fraction=0.1,\n    n_iter_no_change=30,\n    random_state=RANDOM_SEED,\n    verbose=False\n)\n\nfinal_mlp.fit(X_train_val, y_train_val)\nprint(f\"\u2713 Final model trained ({final_mlp.n_iter_} epochs)\")\n\n# -----------------------------------------------\n# STEP 3: Predictions on Test Set\n# -----------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(\"STEP 3: Generating Test Predictions\")\nprint(f\"{'='*70}\")\n\ny_test_pred_scaled = final_mlp.predict(X_test_mlp)\n\n# Inverse transform predictions\nwith open('scaler_y.pkl', 'rb') as f:\n    scaler_y = pickle.load(f)\n\ny_test_mlp_actual = scaler_y.inverse_transform(y_test_mlp.reshape(-1, 1)).ravel()\ny_test_mlp_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()\n\nprint(f\"\u2713 Generated {len(y_test_mlp_pred)} predictions\")\n\n# -----------------------------------------------\n# STEP 4: Evaluation\n# -----------------------------------------------\nmlp_metrics = calculate_metrics(y_test_mlp_actual, y_test_mlp_pred, \"MLP (Optimized)\")\n\n# Save results\nnp.save('mlp_predictions.npy', y_test_mlp_pred)\nwith open('mlp_model_optimized.pkl', 'wb') as f:\n    pickle.dump(final_mlp, f)\n\n# Visualizations\nplot_predictions(y_test_mlp_actual, y_test_mlp_pred, \"MLP (Optimized)\", \"mlp_predictions.png\")\n\n# Plot training history\nplt.figure(figsize=(10, 5))\nplt.plot(final_mlp.loss_curve_, linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('MLP Training Loss Curve')\nplt.grid(True)\nplt.savefig('mlp_training_curve.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n{'='*70}\")\nprint(\"MLP MODEL COMPLETE\")\nprint(f\"{'='*70}\")\nprint(f\"\u2713 Best Architecture: {best_config['name']}\")\nprint(f\"\u2713 MAPE: {mlp_metrics['MAPE']:.2f}%\")\nprint(f\"\u2713 Model saved: mlp_model_optimized.pkl\")\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **ANFIS Model (Enhanced - Target <3% MAPE)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n# ANFIS MODEL - ADAPTIVE NEURO-FUZZY INFERENCE SYSTEM (ENHANCED)\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"ANFIS MODEL - ENHANCED VERSION (PCA Features)\")\nprint(\"=\"*70)\n\n# =============================================================================\n# ANFIS CLASS DEFINITION\n# =============================================================================\n\nclass ANFIS:\n    \"\"\"\n    Enhanced ANFIS with optimizations for better performance\n\n    Key Improvements:\n    - Works with PCA-reduced features (12 instead of 26)\n    - Uses n_mfs=2 for efficiency (2^12 = 4,096 rules instead of 3^26 = 2.3M)\n    - Better training with early stopping\n    - Enhanced initialization\n    \"\"\"\n\n    def __init__(self, n_inputs, n_mfs=2, learning_rate=0.01):\n        self.n_inputs = n_inputs\n        self.n_mfs = n_mfs\n        self.n_rules = n_mfs ** n_inputs\n        self.learning_rate = learning_rate\n\n        print(f\"\\n\u2713 ANFIS initialized:\")\n        print(f\"  Inputs: {n_inputs}\")\n        print(f\"  Membership functions per input: {n_mfs}\")\n        print(f\"  Total fuzzy rules: {self.n_rules:,}\")\n\n        self._initialize_parameters()\n        self.train_losses = []\n        self.val_losses = []\n\n    def _initialize_parameters(self):\n        \"\"\"Initialize membership function and consequent parameters\"\"\"\n\n        # Premise parameters (Gaussian MFs: mean and std)\n        self.premise_params = np.random.randn(self.n_inputs, self.n_mfs, 2) * 0.5\n\n        # Initialize means uniformly\n        for i in range(self.n_inputs):\n            self.premise_params[i, :, 0] = np.linspace(-2, 2, self.n_mfs)\n            self.premise_params[i, :, 1] = np.ones(self.n_mfs) * 1.0\n\n        # Consequent parameters (TSK model)\n        self.consequent_params = np.random.randn(self.n_rules, self.n_inputs + 1) * 0.1\n\n    def _gaussian_mf(self, x, mean, std):\n        \"\"\"Gaussian membership function\"\"\"\n        return np.exp(-0.5 * ((x - mean) / (std + 1e-10)) ** 2)\n\n    def _forward(self, X):\n        \"\"\"\n        Forward pass through ANFIS\n\n        Layer 1: Fuzzification\n        Layer 2: Rule firing strengths\n        Layer 3: Normalization\n        Layer 4: Consequent computation\n        Layer 5: Defuzzification (output)\n        \"\"\"\n        batch_size = X.shape[0]\n\n        # Layer 1: Fuzzification - compute membership degrees\n        memberships = np.zeros((batch_size, self.n_inputs, self.n_mfs))\n        for i in range(self.n_inputs):\n            for j in range(self.n_mfs):\n                mean = self.premise_params[i, j, 0]\n                std = self.premise_params[i, j, 1]\n                memberships[:, i, j] = self._gaussian_mf(X[:, i], mean, std)\n\n        # Layer 2: Rule firing strengths (product of membership degrees)\n        # Generate all combinations of membership functions\n        mf_indices = np.array(list(itertools.product(range(self.n_mfs), repeat=self.n_inputs)))\n\n        firing_strengths = np.ones((batch_size, self.n_rules))\n        for rule_idx in range(self.n_rules):\n            for input_idx in range(self.n_inputs):\n                mf_idx = mf_indices[rule_idx, input_idx]\n                firing_strengths[:, rule_idx] *= memberships[:, input_idx, mf_idx]\n\n        # Layer 3: Normalization\n        firing_sum = np.sum(firing_strengths, axis=1, keepdims=True) + 1e-10\n        normalized_firing = firing_strengths / firing_sum\n\n        # Layer 4: Consequent computation (TSK model)\n        # Extend X with bias term\n        X_extended = np.column_stack([np.ones(batch_size), X])\n\n        # Compute rule outputs\n        rule_outputs = np.dot(X_extended, self.consequent_params.T)\n\n        # Layer 5: Defuzzification (weighted sum)\n        output = np.sum(normalized_firing * rule_outputs, axis=1)\n\n        # Store for backward pass\n        self._cache = {\n            'X': X,\n            'X_extended': X_extended,\n            'memberships': memberships,\n            'firing_strengths': firing_strengths,\n            'normalized_firing': normalized_firing,\n            'rule_outputs': rule_outputs,\n            'mf_indices': mf_indices\n        }\n\n        return output\n\n    def _backward(self, X, y_true, y_pred):\n        \"\"\"Backward pass - update parameters using gradient descent\"\"\"\n\n        batch_size = X.shape[0]\n        error = y_pred - y_true\n\n        # Retrieve cached values\n        X_extended = self._cache['X_extended']\n        normalized_firing = self._cache['normalized_firing']\n\n        # Update consequent parameters (LSE - Least Squares Estimation)\n        # Faster than gradient descent for consequent parameters\n        for rule_idx in range(self.n_rules):\n            w = normalized_firing[:, rule_idx].reshape(-1, 1)\n            weighted_X = X_extended * w\n\n            try:\n                # Solve weighted least squares\n                self.consequent_params[rule_idx] = np.linalg.lstsq(\n                    weighted_X, y_true * w.ravel(), rcond=None\n                )[0]\n            except:\n                # If singular, use gradient descent\n                grad = np.dot(weighted_X.T, error) / batch_size\n                self.consequent_params[rule_idx] -= self.learning_rate * grad\n\n        # Update premise parameters (gradient descent)\n        # Simplified update - full derivation is complex\n        memberships = self._cache['memberships']\n        firing_strengths = self._cache['firing_strengths']\n        mf_indices = self._cache['mf_indices']\n\n        for i in range(self.n_inputs):\n            for j in range(self.n_mfs):\n                # Find rules using this membership function\n                relevant_rules = mf_indices[:, i] == j\n\n                # Compute gradient (simplified)\n                mean = self.premise_params[i, j, 0]\n                std = self.premise_params[i, j, 1]\n\n                mf_values = memberships[:, i, j]\n\n                # Gradient w.r.t. mean\n                grad_mean = np.mean(error * (X[:, i] - mean) / (std**2 + 1e-10) * mf_values)\n\n                # Gradient w.r.t. std\n                grad_std = np.mean(error * ((X[:, i] - mean)**2) / (std**3 + 1e-10) * mf_values)\n\n                # Update with clipping\n                self.premise_params[i, j, 0] -= self.learning_rate * grad_mean\n                self.premise_params[i, j, 1] -= self.learning_rate * grad_std\n\n                # Ensure std stays positive\n                self.premise_params[i, j, 1] = max(0.1, self.premise_params[i, j, 1])\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, max_epochs=300, patience=50, verbose=True):\n        \"\"\"\n        Train ANFIS model\n\n        Parameters:\n        -----------\n        X_train, y_train : Training data\n        X_val, y_val : Validation data (optional, for early stopping)\n        max_epochs : Maximum training epochs\n        patience : Early stopping patience\n        verbose : Print progress\n        \"\"\"\n\n        print(f\"\\n{'='*70}\")\n        print(f\"Training ANFIS\")\n        print(f\"{'='*70}\")\n        print(f\"  Max epochs: {max_epochs}\")\n        print(f\"  Patience: {patience}\")\n        print(f\"  Learning rate: {self.learning_rate}\")\n\n        best_val_loss = np.inf\n        patience_counter = 0\n\n        for epoch in range(max_epochs):\n            # Forward pass\n            y_pred_train = self._forward(X_train)\n\n            # Compute training loss\n            train_loss = np.mean((y_pred_train - y_train) ** 2)\n            self.train_losses.append(train_loss)\n\n            # Backward pass\n            self._backward(X_train, y_train, y_pred_train)\n\n            # Validation\n            if X_val is not None and y_val is not None:\n                y_pred_val = self._forward(X_val)\n                val_loss = np.mean((y_pred_val - y_val) ** 2)\n                self.val_losses.append(val_loss)\n\n                # Early stopping check\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    patience_counter = 0\n                    # Save best parameters\n                    self.best_premise = self.premise_params.copy()\n                    self.best_consequent = self.consequent_params.copy()\n                else:\n                    patience_counter += 1\n\n                if patience_counter >= patience:\n                    print(f\"\\n\u2713 Early stopping at epoch {epoch+1}\")\n                    print(f\"  Best validation loss: {best_val_loss:.6f}\")\n                    # Restore best parameters\n                    self.premise_params = self.best_premise\n                    self.consequent_params = self.best_consequent\n                    break\n\n                if verbose and (epoch + 1) % 20 == 0:\n                    print(f\"  Epoch {epoch+1:3d}/{max_epochs}: \"\n                          f\"Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}\")\n            else:\n                if verbose and (epoch + 1) % 20 == 0:\n                    print(f\"  Epoch {epoch+1:3d}/{max_epochs}: Train Loss={train_loss:.6f}\")\n\n        print(f\"\\n\u2713 Training complete ({len(self.train_losses)} epochs)\")\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Generate predictions\"\"\"\n        return self._forward(X)\n\n\n# =============================================================================\n# LOAD DATA (PCA-TRANSFORMED)\n# =============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"Loading PCA-Transformed Data\")\nprint(f\"{'='*70}\")\n\nX_train_anfis = np.load('X_train_pca.npy')\nX_val_anfis = np.load('X_val_pca.npy')\nX_test_anfis = np.load('X_test_pca.npy')\n\ny_train_anfis = np.load('y_train_scaled.npy')\ny_val_anfis = np.load('y_val_scaled.npy')\ny_test_anfis = np.load('y_test_scaled.npy')\n\nprint(f\"\\n\u2713 Data loaded:\")\nprint(f\"  Train: {X_train_anfis.shape}\")\nprint(f\"  Val:   {X_val_anfis.shape}\")\nprint(f\"  Test:  {X_test_anfis.shape}\")\nprint(f\"\\n\u2713 Using {X_train_anfis.shape[1]} PCA features\")\nprint(f\"  (Dramatically reduces rule complexity!)\")\n\n# =============================================================================\n# TRAIN ANFIS MODEL\n# =============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"Creating and Training ANFIS Model\")\nprint(f\"{'='*70}\")\n\n# Create ANFIS with optimized configuration\nanfis_model = ANFIS(\n    n_inputs=X_train_anfis.shape[1],  # 12 PCA features\n    n_mfs=ANFIS_CONFIG['n_membership_functions'],  # 2 MFs per input\n    learning_rate=ANFIS_CONFIG['learning_rate']  # 0.01\n)\n\n# Train model\nanfis_model.fit(\n    X_train_anfis, y_train_anfis,\n    X_val_anfis, y_val_anfis,\n    max_epochs=ANFIS_CONFIG['max_epochs'],  # 300\n    patience=ANFIS_CONFIG['patience'],  # 50\n    verbose=True\n)\n\n# =============================================================================\n# GENERATE PREDICTIONS\n# =============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"Generating Test Predictions\")\nprint(f\"{'='*70}\")\n\n# Predict on test set\ny_test_pred_scaled_anfis = anfis_model.predict(X_test_anfis)\n\n# Inverse transform\nwith open('scaler_y.pkl', 'rb') as f:\n    scaler_y = pickle.load(f)\n\ny_test_anfis_actual = scaler_y.inverse_transform(y_test_anfis.reshape(-1, 1)).ravel()\ny_test_anfis_pred = scaler_y.inverse_transform(y_test_pred_scaled_anfis.reshape(-1, 1)).ravel()\n\nprint(f\"\u2713 Generated {len(y_test_anfis_pred)} predictions\")\n\n# =============================================================================\n# EVALUATE MODEL\n# =============================================================================\n\nanfis_metrics = calculate_metrics(y_test_anfis_actual, y_test_anfis_pred, \"ANFIS (Enhanced)\")\n\n# Save results\nnp.save('anfis_predictions.npy', y_test_anfis_pred)\nwith open('anfis_model_enhanced.pkl', 'wb') as f:\n    pickle.dump(anfis_model, f)\n\n# =============================================================================\n# VISUALIZATIONS\n# =============================================================================\n\n# Prediction plots\nplot_predictions(y_test_anfis_actual, y_test_anfis_pred, \"ANFIS (Enhanced)\", \"anfis_predictions.png\")\n\n# Training history\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nepochs = range(1, len(anfis_model.train_losses) + 1)\nax.plot(epochs, anfis_model.train_losses, label='Training Loss', linewidth=2)\nif anfis_model.val_losses:\n    ax.plot(epochs, anfis_model.val_losses, label='Validation Loss', linewidth=2)\nax.set_xlabel('Epoch')\nax.set_ylabel('MSE Loss')\nax.set_title('ANFIS Training History')\nax.legend()\nax.grid(True)\nplt.savefig('anfis_training_curve.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n{'='*70}\")\nprint(\"ANFIS MODEL COMPLETE\")\nprint(f\"{'='*70}\")\nprint(f\"\u2713 Rules: {anfis_model.n_rules:,} (reduced from 2.3M with original features)\")\nprint(f\"\u2713 Epochs trained: {len(anfis_model.train_losses)}\")\nprint(f\"\u2713 MAPE: {anfis_metrics['MAPE']:.2f}%\")\n\nif anfis_metrics['MAPE'] < 3.0:\n    print(f\"\\n\ud83c\udfaf TARGET ACHIEVED: MAPE < 3% !\")\nelse:\n    print(f\"\\n\u26a0 Target: <3% MAPE (Current: {anfis_metrics['MAPE']:.2f}%)\")\n\nprint(f\"\u2713 Model saved: anfis_model_enhanced.pkl\")\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}